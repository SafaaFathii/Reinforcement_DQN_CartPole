{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SafaaFathii/Reinforcement_DQN_CartPole/blob/main/Reinforcement_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCZ2tQFYr0m6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gym is a standard API for reinforcement learning, and a diverse collection of reference environments."
      ],
      "metadata": {
        "id": "bV0ymKAJPx3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cFUPZdCM4dm",
        "outputId": "8ef6edb2-a9dd-4161-ad2f-ec4b1af091ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "# install gym\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EwzUQMW4NGk8",
        "outputId": "47b8316f-69d9-4593-c8c1-4745983c90d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 994 kB of archives.\n",
            "After this operation, 2,982 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.11 [785 kB]\n",
            "Fetched 994 kB in 1s (1,054 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 155673 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.11_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [87.8 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,527 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,905 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,306 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,336 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,063 kB]\n",
            "Fetched 11.5 MB in 4s (2,791 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 55 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 0s (2,094 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 155734 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.2.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-63.3.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-22.2.1 setuptools-63.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ale-py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4) (4.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4) (1.21.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4) (3.8.1)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.4/256.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabgymrender==1.0.2\n",
            "  Downloading colabgymrender-1.0.2.tar.gz (1.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (0.2.3.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (0.17.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from colabgymrender==1.0.2) (4.6.0.66)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->colabgymrender==1.0.2) (1.5.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender==1.0.2) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender==1.0.2) (4.64.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender==1.0.2) (7.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->colabgymrender==1.0.2) (0.16.0)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.0.2-py3-none-any.whl size=2426 sha256=b04ec18dea1709c2397c8acfa62ec4d2d7f796b46b33eb3da35cc6b7cc9717ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/56/73/3697080da5fc7b120516aef37d1d1eb2380515ba9e272b8ccd\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.0.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install dependencies needed for recording videos\n",
        "!apt install -y xvfb x11-utils\n",
        "!sudo apt-get update\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install opencv-python\n",
        "\n",
        "!pip install pyglet\n",
        "!pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)\n",
        "!pip install pickle5\n",
        "\n",
        "#!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "#!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AvI0JUHouYM0"
      },
      "outputs": [],
      "source": [
        "!export DISPLAY=localhost:0.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "zbrGgYzdP7yK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jMVy7aZrRhOr"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import gym # for environment\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from keras.optimizers import Adam # adaptive momentum \n",
        "import random\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SmKYpfY9vQYT"
      },
      "outputs": [],
      "source": [
        "# start an instance of the virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VWTanVzHvUhN"
      },
      "outputs": [],
      "source": [
        "# video display helper function\n",
        "from base64 import b64encode\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJM9Q3VVr_tR"
      },
      "source": [
        "#CartPole Env"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Using some build-in games in Gym that do not require additonal installs. Let's start with a very basic game called CartPole."
      ],
      "metadata": {
        "id": "59ZxeOioQudu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEeXw8mVrmKU",
        "outputId": "fdffd8c0-2a1a-4517-dba0-b16a2849cba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "The action space: Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "# Observation and action space \n",
        "obs_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "print(f\"The observation space: {obs_space}\")\n",
        "print(f\"The action space: {action_space}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CartPole Game"
      ],
      "metadata": {
        "id": "iwVoFFPdRjEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   CartPole is one of the most straightforward environments in OpenAI gym.\n",
        "*   Cartpole is built on a Markov chain model.\n",
        "*   The goal of CartPole is to balance a pole connected with one joint on top of a moving cart. An agent can move the cart by performing a series of 0 or 1 actions, pushing it left or right.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LYfKFWDVRmm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dkaSDrNsJT8",
        "outputId": "081f161e-8e73-44ec-dd09-dfdce0b4a0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The initial observation is [-0.0349232  -0.04472921  0.02384931 -0.03763041]\n",
            "The new observation is [-0.03581778 -0.2401849   0.0230967   0.26248078]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# reset the environment and see the initial observation\n",
        "obs = env.reset()\n",
        "print(\"The initial observation is {}\".format(obs))\n",
        "\n",
        "# Sample a random action from the entire action space\n",
        "random_action = env.action_space.sample()\n",
        "\n",
        "# # Take the action and get the new observation space\n",
        "new_obs, reward, done, info = env.step(random_action)\n",
        "print(\"The new observation is {}\".format(new_obs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiEuBukcwmLz",
        "outputId": "21f34336-f1a4-433e-cce2-8a2d2f4ba123"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "env.env.s=42 # some random number, you might recognize it\n",
        "env.render()\n",
        "env.env.s = 222 # and some other\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSfBxaHOY9iB",
        "outputId": "d8e9a0b1-2cba-48d1-aa44-f2773a87b859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    env.render()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class accepts a function that returns an environment, and returns a vectorized version of the environment. It essentially generates \n",
        "n\n",
        " copies of the environment. Its step function expects a vector of \n",
        "n\n",
        " actions, and returns vectors of \n",
        "n\n",
        " next states, \n",
        "n\n",
        " rewards, \n",
        "n\n",
        " done flags, and \n",
        "n\n",
        " infos."
      ],
      "metadata": {
        "id": "_4eUvfEJdmJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YZfFsydDY-Oy"
      },
      "outputs": [],
      "source": [
        "class VectorizedEnvWrapper(gym.Wrapper):\n",
        "    def __init__(self, make_env, num_envs=1):\n",
        "        super().__init__(make_env())\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = [make_env() for env_index in range(num_envs)]\n",
        "    \n",
        "    def reset(self):\n",
        "        return np.asarray([env.reset() for env in self.envs])\n",
        "    \n",
        "    def reset_at(self, env_index):\n",
        "        return self.envs[env_index].reset()\n",
        "    \n",
        "    def step(self, actions):\n",
        "        next_states, rewards, dones, infos = [], [], [], []\n",
        "        for env, action in zip(self.envs, actions):\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_states.append(next_state)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            infos.append(info)\n",
        "        return np.asarray(next_states), np.asarray(rewards), \\\n",
        "            np.asarray(dones), np.asarray(infos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bOb_HEnZ2Gq",
        "outputId": "8da71a3d-0be2-4217-a9a0-19e0da48ec73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 4)\n",
            "(128,)\n",
            "(128,)\n"
          ]
        }
      ],
      "source": [
        "num_envs = 128\n",
        "env = VectorizedEnvWrapper(lambda: gym.make(\"CartPole-v1\"), num_envs=num_envs)\n",
        "T = 10\n",
        "observations = env.reset()\n",
        "for t in range(T):\n",
        "    actions = np.random.randint(env.action_space.n, size=num_envs)\n",
        "    observations, rewards, dones, infos = env.step(actions)  \n",
        "    for i in range(len(dones)):\n",
        "        if dones[i]:\n",
        "            observations[i] = env.reset_at(i)\n",
        "print(observations.shape)\n",
        "print(rewards.shape)\n",
        "print(dones.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGFAVP9hbfPl"
      },
      "source": [
        "#DQN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zj5Fwn1WbQmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "cB2KtxKsbpXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a NN to understand and predict based on the environment data, we have initialized our model and feed it to the information. Then the model will train on those data to approximate the output based on the input.\n",
        " I used three layers of Neural Network, 64, 64, and 64 neurons."
      ],
      "metadata": {
        "id": "iTij-NCGbSAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class DQNAgent"
      ],
      "metadata": {
        "id": "kI3H5pZ1br0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remember Function"
      ],
      "metadata": {
        "id": "_L9Bb5wlb79W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network used in the algorithm tends to forget the previous experiences as it overwrites them with new experiences. So, we need a memory (list) of previous experiences and observations to re-train the model with the earlier experiences. We will call this array of experiences memory and use a remember() function to append state, action, reward, and next state to the memory."
      ],
      "metadata": {
        "id": "fUrLfN-1cPQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Replay Function"
      ],
      "metadata": {
        "id": "ja5rRUaMctN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " First, we will sample some experiences from the memory and call them minibatch. If the memory size is less than 64, we will take everything is in our memory."
      ],
      "metadata": {
        "id": "Canc6tjifruy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run Function"
      ],
      "metadata": {
        "id": "y3bpLQEQgrCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " we are running for 1000 episodes of the game to train.We save results from every step to memory, which we use for training on every step.When our model hits a score of 500, we keep it, and already we can use it for testing. "
      ],
      "metadata": {
        "id": "6ZdmcYqVhBHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OcloHWMCsaOf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "\n",
        "def OurModel(input_shape, action_space):\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # 'Dense' is the basic form of a neural network layer\n",
        "    # Input Layer of state size(4) and Hidden Layer with 64 nodes\n",
        "    X = Dense(64, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
        "\n",
        "    # Hidden layer with 64 nodes\n",
        "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "    \n",
        "    # Hidden layer with 64 nodes\n",
        "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    # Output Layer with # of actions: 2 nodes (left, right)\n",
        "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name='CartPole_DQN_model')\n",
        "    model.compile(loss=\"mse\", optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        # by default, CartPole-v1 has max episode steps = 500\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES = 1000\n",
        "        self.memory = deque(maxlen=3000)\n",
        "        \n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.batch_size = 64\n",
        "        self.train_start = 1000\n",
        "\n",
        "        # create main model\n",
        "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.train_start:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            return np.argmax(self.model.predict(state))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.train_start:\n",
        "            return\n",
        "        # Randomly sample minibatch from the memory\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
        "\n",
        "        state = np.zeros((self.batch_size, self.state_size))\n",
        "        next_state = np.zeros((self.batch_size, self.state_size))\n",
        "        action, reward, done = [], [], []\n",
        "\n",
        "        # do this before prediction\n",
        "        # for speedup, this could be done on the tensor level\n",
        "        # but easier to understand using a loop\n",
        "        for i in range(self.batch_size):\n",
        "            state[i] = minibatch[i][0]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            done.append(minibatch[i][4])\n",
        "\n",
        "        # do batch prediction to save speed\n",
        "        target = self.model.predict(state)\n",
        "        target_next = self.model.predict(next_state)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            # correction on the Q value for the action used\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                # Standard - DQN\n",
        "                # DQN chooses the max Q value among next actions\n",
        "                # selection and evaluation of action is on the target Q Network\n",
        "                # Q_max = max_a' Q_target(s', a')\n",
        "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
        "\n",
        "        # Train the Neural Network with batches\n",
        "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model = load_model(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)\n",
        "            \n",
        "    def run(self):\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = np.reshape(next_state, [1, self.state_size])\n",
        "                if not done or i == self.env._max_episode_steps-1:\n",
        "                    reward = reward\n",
        "                else:\n",
        "                    reward = -100\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "                if done:                   \n",
        "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, self.epsilon))\n",
        "                    if i == 500:\n",
        "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
        "                        self.save(\"cartpole-dqn.h5\")\n",
        "                        return\n",
        "                self.replay()\n",
        "\n",
        "    def test(self):\n",
        "        self.load(\"cartpole-dqn.h5\")\n",
        "        right_pred = 0\n",
        "        for e in range(20):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                action = np.argmax(self.model.predict(state))\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                state = np.reshape(next_state, [1, self.state_size])\n",
        "                i += 1\n",
        "                if done:\n",
        "                    print(\"episode: {}/{}, score: {}\".format(e, 20, i))\n",
        "                    if i==500:\n",
        "                        right_pred+=1\n",
        "                    break\n",
        "        print(\"{}/{} are equal to 500\".format(right_pred,20))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Model"
      ],
      "metadata": {
        "id": "azNHOyEPhneA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_guxbbefBYM",
        "outputId": "8a376740-ce25-4962-ae3a-2cdb723b7a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"CartPole_DQN_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 4)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                320       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,770\n",
            "Trainable params: 8,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/1000, score: 16, e: 1.0\n",
            "episode: 1/1000, score: 22, e: 1.0\n",
            "episode: 2/1000, score: 20, e: 1.0\n",
            "episode: 3/1000, score: 25, e: 1.0\n",
            "episode: 4/1000, score: 14, e: 1.0\n",
            "episode: 5/1000, score: 11, e: 1.0\n",
            "episode: 6/1000, score: 29, e: 1.0\n",
            "episode: 7/1000, score: 16, e: 1.0\n",
            "episode: 8/1000, score: 30, e: 1.0\n",
            "episode: 9/1000, score: 46, e: 1.0\n",
            "episode: 10/1000, score: 22, e: 1.0\n",
            "episode: 11/1000, score: 13, e: 1.0\n",
            "episode: 12/1000, score: 21, e: 1.0\n",
            "episode: 13/1000, score: 22, e: 1.0\n",
            "episode: 14/1000, score: 16, e: 1.0\n",
            "episode: 15/1000, score: 15, e: 1.0\n",
            "episode: 16/1000, score: 19, e: 1.0\n",
            "episode: 17/1000, score: 32, e: 1.0\n",
            "episode: 18/1000, score: 15, e: 1.0\n",
            "episode: 19/1000, score: 20, e: 1.0\n",
            "episode: 20/1000, score: 15, e: 1.0\n",
            "episode: 21/1000, score: 13, e: 1.0\n",
            "episode: 22/1000, score: 32, e: 1.0\n",
            "episode: 23/1000, score: 48, e: 1.0\n",
            "episode: 24/1000, score: 28, e: 1.0\n",
            "episode: 25/1000, score: 13, e: 1.0\n",
            "episode: 26/1000, score: 13, e: 1.0\n",
            "episode: 27/1000, score: 14, e: 1.0\n",
            "episode: 28/1000, score: 14, e: 1.0\n",
            "episode: 29/1000, score: 12, e: 1.0\n",
            "episode: 30/1000, score: 25, e: 1.0\n",
            "episode: 31/1000, score: 13, e: 1.0\n",
            "episode: 32/1000, score: 15, e: 1.0\n",
            "episode: 33/1000, score: 15, e: 1.0\n",
            "episode: 34/1000, score: 21, e: 1.0\n",
            "episode: 35/1000, score: 13, e: 1.0\n",
            "episode: 36/1000, score: 18, e: 1.0\n",
            "episode: 37/1000, score: 16, e: 1.0\n",
            "episode: 38/1000, score: 18, e: 1.0\n",
            "episode: 39/1000, score: 15, e: 1.0\n",
            "episode: 40/1000, score: 22, e: 1.0\n",
            "episode: 41/1000, score: 10, e: 1.0\n",
            "episode: 42/1000, score: 37, e: 1.0\n",
            "episode: 43/1000, score: 10, e: 1.0\n",
            "episode: 44/1000, score: 19, e: 1.0\n",
            "episode: 45/1000, score: 11, e: 1.0\n",
            "episode: 46/1000, score: 13, e: 1.0\n",
            "episode: 47/1000, score: 19, e: 1.0\n",
            "episode: 48/1000, score: 21, e: 1.0\n",
            "episode: 49/1000, score: 12, e: 1.0\n",
            "episode: 50/1000, score: 39, e: 0.99\n",
            "episode: 51/1000, score: 25, e: 0.97\n",
            "episode: 52/1000, score: 35, e: 0.93\n",
            "episode: 53/1000, score: 13, e: 0.92\n",
            "episode: 54/1000, score: 24, e: 0.9\n",
            "episode: 55/1000, score: 89, e: 0.82\n",
            "episode: 56/1000, score: 43, e: 0.79\n",
            "episode: 57/1000, score: 36, e: 0.76\n",
            "episode: 58/1000, score: 18, e: 0.75\n",
            "episode: 59/1000, score: 32, e: 0.72\n",
            "episode: 60/1000, score: 19, e: 0.71\n",
            "episode: 61/1000, score: 63, e: 0.67\n",
            "episode: 62/1000, score: 38, e: 0.64\n",
            "episode: 63/1000, score: 24, e: 0.63\n",
            "episode: 64/1000, score: 9, e: 0.62\n",
            "episode: 65/1000, score: 34, e: 0.6\n",
            "episode: 66/1000, score: 67, e: 0.56\n",
            "episode: 67/1000, score: 71, e: 0.52\n",
            "episode: 68/1000, score: 15, e: 0.52\n",
            "episode: 69/1000, score: 21, e: 0.5\n",
            "episode: 70/1000, score: 40, e: 0.48\n",
            "episode: 71/1000, score: 116, e: 0.43\n",
            "episode: 72/1000, score: 24, e: 0.42\n",
            "episode: 73/1000, score: 168, e: 0.36\n",
            "episode: 74/1000, score: 79, e: 0.33\n",
            "episode: 75/1000, score: 52, e: 0.31\n",
            "episode: 76/1000, score: 90, e: 0.29\n",
            "episode: 77/1000, score: 58, e: 0.27\n",
            "episode: 78/1000, score: 78, e: 0.25\n",
            "episode: 79/1000, score: 70, e: 0.23\n",
            "episode: 80/1000, score: 117, e: 0.21\n",
            "episode: 81/1000, score: 137, e: 0.18\n",
            "episode: 82/1000, score: 500, e: 0.11\n",
            "Saving trained model as cartpole-dqn.h5\n"
          ]
        }
      ],
      "source": [
        "agent = DQNAgent()\n",
        "agent.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Model"
      ],
      "metadata": {
        "id": "GkFkW6MehqP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here are 20 test episodes of our trained model. As you can see, some of it hit the maximum score and the rest are really close to 500, it would be interesting what is the maximum score it could beat, but sadly limit is 500"
      ],
      "metadata": {
        "id": "6HyHPeXVlnwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znqHsTtcss-n",
        "outputId": "cf2cc5c0-722d-408e-bac0-55c1231ef05c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/20, score: 352\n",
            "episode: 1/20, score: 452\n",
            "episode: 2/20, score: 387\n",
            "episode: 3/20, score: 441\n",
            "episode: 4/20, score: 395\n",
            "episode: 5/20, score: 386\n",
            "episode: 6/20, score: 178\n",
            "episode: 7/20, score: 398\n",
            "episode: 8/20, score: 464\n",
            "episode: 9/20, score: 340\n",
            "episode: 10/20, score: 500\n",
            "episode: 11/20, score: 427\n",
            "episode: 12/20, score: 248\n",
            "episode: 13/20, score: 178\n",
            "episode: 14/20, score: 459\n",
            "episode: 15/20, score: 386\n",
            "episode: 16/20, score: 403\n",
            "episode: 17/20, score: 381\n",
            "episode: 18/20, score: 353\n",
            "episode: 19/20, score: 500\n",
            "2/20 are equal to 500\n"
          ]
        }
      ],
      "source": [
        "agent.test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IXeQXZ88VDOT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Reinforcement_Final_Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM5U8dldVoYjKqk98lhoJMu",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}